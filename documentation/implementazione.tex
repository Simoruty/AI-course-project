\section{Implementazione}

\subsection{Architettura del sistema}
Per preservare l'indipendenza delle componenti del sistema, abbiamo realizzato dei moduli Prolog che espongano pubblicamente solo i predicati necessari.

Il file \verb+main.pl+ costituisce l'entry-point del programma, l'unico file necessario da importare nell'interprete per l'esecuzione.


\subsection{Lexer}
Il compito del \emph{lexer} sarà quello di normalizzare la stringa contenente il documento da analizzare in una lista di token su cui poi il tagger dovrà fare le sue analisi. Per fare ciò, bisognerà nell'ordine:
\begin{itemize}
    \item ripulire la stringa di stopchars;
    \item separare i caratteri speciali (punto, virgola, euro e chiocciola) da eventuali caratteri a cui sono associati;
    \item eliminare eventuali spazi bianchi superflui causati da eliminazioni di caratteri fatte in precedenza;
    \item rendere case insensitive la stringa ripulita.
\end{itemize}

Per quanto riguarda la pulizia degli stopchar ininfluenti ai fini del tag, si è deciso di eliminare i caratteri quali il punto interrogativo, il punto esclamativo, virgolette, apici, punti e virgola, due punti e parentesi tonde e graffe.

Alcuni caratteri speciali sono stati preservati e separati dai token a cui sono uniti in modo da diventare essi stessi dei nuovi token. Questo perché tali caratteri hanno una semantica in alcune regole di livello più alto.
Ad esempio il carattere speciale euro (€) risulta essere molto utile per l'identificazione di richieste di denaro all'interno del documento.

Di seguito, a titolo esemplificativo, il codice del predicato lexer:

\begin{prologcode}
lexer(String, ListToken) :-
   strip_useless_chars(String, Temp1),
   separate_useful_chars(Temp1, Temp2),
   strip_spaces(Temp2, Temp3),
   atom_codes(Temp4, Temp3),
   atomic_list_concat(Temp5, ' ', Temp4),
   maplist(downcase_atom, Temp5, Temp6),
   strip_sep(Temp6, ListToken).
\end{prologcode}

\subsection{Creazione dei token}
Dopo l'elaborazione descritta nella sezione precedente, il sistema si occupa di creare un identificatore per ogni token, nella forma \verb:`tok'+Num: con \verb+Num+ intero positivo crescente.

Per ognuno dei token trovati, verranno asseriti alcuni fatti che rappresentano la base di conoscenza sulla quale le regole cercheranno i vari tag ai quali siamo interessati.

Di seguito un esempio esplicativo dei fatti asseriti.

\begin{prologcode}
token('tok1', 'curatore').
token('tok2', 'fallimentare').
token('tok3', 'tribunale').
token('tok4', 'milano').
next('tok1', 'tok2').
next('tok2', 'tok3').
next('tok3', 'tok4').
\end{prologcode}


\subsection{Gestione di più documenti}
Il sistema è in grado di lavorare su diversi documenti di input senza conflitti o interazioni.

Per farlo abbiamo introdotto un nuovo predicato, \verb+appartiene/2+ e due nuovi token per ogni documento: \verb+BOF+ e \verb+EOF+.
Ad ogni documento passato in input, viene assegnato un identificatore, nella forma \verb:`doc'+Num:. Inoltre vengono generati due nuovi token fittizi, chiamati \verb+BOF+ (Begin Of File) e \verb+EOF+ (End Of File), rispettivamente token di inizio e file documento.

Per ognuno dei token ritrovati nella stringa del documento, oltre ai 2 fittizi, viene inoltre asserita l'appartenenza al documento.

Nel pezzo di codice seguente sono riportati degli esempi.

\begin{prologcode}
documento('doc0', "Questo e' un esempio").
token('tok0', 'doc0_BOF').
token('tok5', 'doc0_EOF').

appartiene('tok0', 'doc0').
appartiene('tok1', 'doc0').
appartiene('tok2', 'doc0').
appartiene('tok3', 'doc0').
appartiene('tok4', 'doc0').
appartiene('tok5', 'doc0').

next('tok0', 'tok1').
next('tok4', 'tok5').
\end{prologcode}

\subsection{Creazione dei tag}

