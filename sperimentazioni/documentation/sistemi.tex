\section{Sistemi di apprendimento}

I sistemi di apprendimento presi in esame saranno di tre tipi differenti:

\begin{itemize}
	\item Progol 
	\item ALEPH (A Learning Engine for Proposing Hypothesis)
	\item FOIL (First Order Inductive Learner)
\end{itemize}

\subsection{Progol}
Progol è un sistema di ILP usato nell'ambito del machine learning che combina la tecnica della "Inverse Entailment" con la general-to-specific search" attraverso l'utilizzo di un grafo raffinato.
L'inverse Entailment viene utilizzata per ottenere la clausola più specifica a partire dall'esempio dato; inoltre questa clausola verrà usata per migliorare la ricerca all'interno del grafo raffinato creato.
L'approccio utilizzato per la realizzazione dell'implementazione inversa prevede tre fasi:
\begin{enumerate}
\item Selezione casuale del seed (esempio positivo p)
\item Definizione dello spazio delle possibili clausole che potrebbero implementare l'esempio
	\begin{itemize}
		\item Genera la clausola più bassa
		\item Conterrà tutti i letterali definiti nella KB che potrebbero coprire l'esempio p
	\end{itemize}
\item Ricerca in questo spazio 
\end{enumerate}

A differenza di FOIL, la ricerca in progol è molto più efficiente ed è possibile dimostrare che restituirà come risultato la soluzione con la più alta compressione nello spazio di ricerca 
PROGOL è interessante per diversi motivi. primo, ha adottato l'idea dell'algoritmo AQ della ILP. Come AQ, seleziona un esempio come seed e computa una generalizzazione minimale dell'esempio in base sia alla knowledge base disponibile che alla profondità massima di inferenza settata. la bottom rule risultante è il modo più usato per limitare la spazio di ricerca per una successiva ricerca top-down per la miglior generalizzazione.
Secondo, a differenza di AQ, Progol non usa una ricerca euristica per trovare la miglior generalizzazione dell'esempio seed, ma usa una variante della ricerca best-first A*.
\nocite{wiki:progol}
%WIKIPEDIA 
P-Progol is intended to be a prototype for exploring ideas. It commenced in 1993 as part of a fun project undertaken by Ashwin Srinivasan and Rui Camacho at Oxford University. The main purpose was to understand ideas of inverse entailment which eventually appeared in Steve Muggleton's paper, and was accompanied by (good-natured) bun-fights about the execution speeds of programs being written by Ashwin (who was writing P-Progol) and Rui (who was writing Indlog). The earliest P-Progol implementation dates from April, 1993. There are, currently, at least 3 implementations based on the Progol algorithm: CProgol (by S. Muggleton, written in C which contains its own Prolog interpreter), Indlog (by Rui Camacho, in Prolog requiring the Yap compiler) and P-Progol (by Ashwin Srinivasan, in Prolog largely developed with Yap). The main differences in implementation (other than language) are in the search technique used and degree of user-interaction allowed

The basic P-Progol algorithm
During routine use, P-Progol follows a very simple procedure that can be described in 4 steps:
Select example. Select an example to be generalised. If none exist, stop, otherwise proceed to the next step.
Build most-specific-clause. Construct the most specific clause that entails the example selected, and is within language restrictions provided. This is usually a definite clause with many literals, and is called the "bottom clause." This step is sometimes called the "saturation" step.
Search. Find a clause more general than the bottom clause. This is done by searching for some subset of the literals in the bottom clause that has the "best" score. Two points should be noted. First, confining the search to subsets of the bottom clause does not produce all the clauses more general than it, but is good enough for this thumbnail sketch. Second, the exact nature of the score of a clause is not really important here. This step is sometimes called the "reduction" step.
Remove redundant. The clause with the best score is added to the current theory, and all examples made redundant are removed. This step is sometimes called the "cover removal" step. Note here that the best clause may make clauses other than the examples redundant. Again, this is ignored here. Return to Step 1.
P-Progol is an implementation based around these 4 steps. A more advanced use of P-Progol (see section Advanced use of P-Progol) allows alteration to each of these steps.

\subsection{ALEPH}
Aleph, acronimo di "\emph{\textbf{A} \textbf{L}earning \textbf{E}ngine for \textbf{P}roposing \textbf{H}ypotheses}", è un sistema di apprendimento ILP (Inductive Logic Programming), il cui scopo principale è quello di  definire il concetto di implicazione inversa.
A differenza di FOIL, questo sistema si basa su una strategia ibrida atta sia generalizzare che specializzare le clausole di Horn. 

L'approccio utilizzato per realizzare l'inverse entailment è lo stesso definito per PROGOL.

Si mostra di seguito il funzionamento dell’algoritmo base di Aleph:
Per ogni esempio da generalizzare;
Scegli un esempio;
Costruisci la clausola più specifica (bottom clause);
Trova una clausola più generale della bottom clause costruendo
sottoinsiemi di letterali della bottom clause che hanno il
miglior punteggio (Punteggio calcolato da Aleph);
Aggiungi la clausola con il miglior punteggio alla teoria e
rimuovi gli esempi ridondanti;
Ripeti;

ALEPH inoltre possiede una gestione del Test Set leggermente diversa rispetto a FOIL. Infatti una volta avviato il sistema e caricata la background knowledge ALEPH costruisce la sua teoria sugli esempi di training e successivamente gli si forniscono i documenti di test per valutare i risultati dell’apprendimento. Su FOIL invece questo non è possibile, il sistema viene avviato dandogli in input sia il test set che il training set e fornisce direttamente i risultati. Maggiori dettagli sono approfonditi nel capitolo successivo relativo alla fase di riscrittura dei dataset e nel capitolo relativo alla conduzione dell’esperimento vero e proprio.

During routine use, Aleph follows a very simple procedure that can be described in 4 steps:

Select example. Select an example to be generalised. If none exist, stop, otherwise proceed to the next step.
Build most-specific-clause. Construct the most specific clause that entails the example selected, and is within language restrictions provided. This is usually a definite clause with many literals, and is called the "bottom clause." This step is sometimes called the "saturation" step. Details of constructing the bottom clause can be found in Stephen Muggleton's 1995 paper: Inverse Entailment and Progol, New Gen. Comput., 13:245-286, available at 

Search. Find a clause more general than the bottom clause. This is done by searching for some subset of the literals in the bottom clause that has the "best" score. Two points should be noted. First, confining the search to subsets of the bottom clause does not produce all the clauses more general than it, but is good enough for this thumbnail sketch. Second, the exact nature of the score of a clause is not really important here. This step is sometimes called the "reduction" step.
Remove redundant. The clause with the best score is added to the current theory, and all examples made redundant are removed. This step is sometimes called the "cover removal" step. Note here that the best clause may make clauses other than the examples redundant. Again, this is ignored here. Return to Step 1.
A more advanced use of Aleph (see section Advanced use of Aleph) allows alteration to each of these steps. At the core of Aleph is the "reduction" step, presented above as a simple "subset-selection" algorithm. In fact, within Aleph, this is implemented by a (restricted) branch-and-bound algorithm which allows an intelligent enumeration of acceptable clauses under a range of different conditions. More on this can be found in section On how the single-clause search is implemented.



Aleph requires three files to construct theories. They are “filestem.b, filestem.f, and filestem.n,” which stand for background knowledge file, positive example file, and negative example file, respectively. Aleph receives these three files in order to construct a theory that is consistent with positive examples and inconsistent with negative ones.
The background knowledge inputted into Aleph is generated by means of Case-Based Reasoning (CBR) with user interaction if necessary. Sophisticated background knowledge should be provided to efficiently process natural language documents since they are rich in representation. The acquisition of background knowledge is one of the problems to be overcome. On the other hand, the acquisition and compilation of background knowledge have been reported to cause a bottleneck in knowledge engineering because they are very time-consuming tasks. The automatic generation of background knowledge provides a solution to the problem.

\subsection{FOIL}
\nocite{Quinlan:1993:FMR:645323.649599}
FOIL è un sistema di apprendimento in grado di costruire delle clausole di Horn su una relazione target partendo sia dalla relazione stessa che da altre relazioni. Il sistema è in realtà un po' più flessibile in quanto può apprendere diverse relazioni in sequenza, permettendo sia l'utilizzo di letterali negati all'interno delle definizioni (usando la semantica Prolog), sia l'impiego di alcune costanti all'interno delle definizioni prodotte dal sistema stesso.
FOIL si basa sulla tecnica del \textit{separate-and-conquer}; tecnica basata essenzialmente su due fasi, nella fase di \textit{separate} si andrà a creare una regola che sia in grado di coprire il maggior numero di esempi positivi presenti nel training set; successivamente si andrà a continuare in maniera ricorsiva nella creazione di regola fintanto che non ci saranno più esempi positivi da coprire (fase di conquer).
La metrica utilizzata per definire la regola migliore in termini di copertura sarà una metrica basata sulla teoria dell'informazione come ad esempio il valore entropico che la regola avrà sul training.
Inoltre, FOIL si basa anche sul concetto di CWA, nel senso che, qualora non dovesse avere a disposizione degli esempi negativi su cui andare ad indurre le regole da creare, assumerà che qualunque altro caso o esempio al di fuori di quelli presenti nel training sono da considerarsi negativi (ipotesi del mondo chiuso).
\begin{algorithm}
	\begin{algorithmic}
	\REQUIRE Lista di esempi
	\FORALL {esempi $\in$ Training Set}
	\STATE Pos $\Rightarrow$ contiene tutti gli esempi positivi
	\STATE Pred $\Rightarrow$ contiene il predicato da apprendere
	\WHILE{Pos non è vuoto}
	\STATE Neg $\Rightarrow$ contiene tutti gli esempi negativi
	\STATE Impostare \textit{body} a vuoto
	\WHILE {Neg non è vuoto}
	\STATE Scegliere un letterale L
	\STATE Aggiungere L a Body
	\STATE Rimuovere gli esempi negativi che non soddisfano L
	\ENDWHILE
	\STATE Aggiungere il predicato \textit{pred} al \textit{body}
	\STATE Rimuovere da Pos tutti gli esempi che soddisfano body
	\ENDWHILE
	\ENDFOR
	\RETURN regola nella logica del primo ordine
	\end{algorithmic}
\end{algorithm}