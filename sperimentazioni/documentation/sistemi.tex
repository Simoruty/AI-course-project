\section{Sistemi di apprendimento}

I sistemi di apprendimento presi in esame saranno di tre tipi differenti:

\begin{itemize}
	\item ALEPH (A Learning Engine for Proposing Hypothesis)
	\item Progol 
	\item FOIL (First Order Inductive Learner)
\end{itemize}

\subsection{ALEPH}
Aleph is an acronym for "A Learning Engine for Proposing Hypotheses" and an Inductive Logic Programming (ILP) system. Aleph’s main purpose was to understand the concept of inverse entailment, which appeared in [2]. Aleph requires three files to construct theories. They are “filestem.b, filestem.f, and filestem.n,” which stand for background knowledge file, positive example file, and negative example file, respectively. Aleph receives these three files in order to construct a theory that is consistent with positive examples and inconsistent with negative ones.
The background knowledge inputted into Aleph is generated by means of Case-Based Reasoning (CBR) with user interaction if necessary. Sophisticated background knowledge should be provided to efficiently process natural language documents since they are rich in representation. The acquisition of background knowledge is one of the problems to be overcome. On the other hand, the acquisition and compilation of background knowledge have been reported to cause a bottleneck in knowledge engineering because they are very time-consuming tasks. The automatic generation of background knowledge provides a solution to the problem.

\subsection{Progol}
Progol is an implementation of Inductive Logic Programming used in computer science that combines "Inverse Entailment" with "general-to-specific search" through a refinement graph. [1][2][3] "Inverse Entailment" is used with mode declarations to derive the most-specific clause within the mode language which entails a given example. This clause is used to guide a refinement-graph search.

Unlike the searches of Ehud Shapiro's Model Inference System (MIS) and J. Ross Quinlan's FOIL Progol's search is efficient and has a provable guarantee of returning a solution having the maximum "compression" in the search-space. To do so it performs an admissible A*-like search, guided by compression, over clauses which subsume the most specific clause.

Progol deals with noisy data by using the "compression measure" to trade-off the description of errors against the hypothesis description length. Progol allows arbitrary Prolog programs as background knowledge and arbitrary definite clauses as examples. Despite this bench-tests show that the efficiency of Progol compares favourably with FOIL.
\nocite{wiki:progol}
%WIKIPEDIA 

P-Progol is intended to be a prototype for exploring ideas. It commenced in 1993 as part of a fun project undertaken by Ashwin Srinivasan and Rui Camacho at Oxford University. The main purpose was to understand ideas of inverse entailment which eventually appeared in Steve Muggleton's paper, and was accompanied by (good-natured) bun-fights about the execution speeds of programs being written by Ashwin (who was writing P-Progol) and Rui (who was writing Indlog). The earliest P-Progol implementation dates from April, 1993. There are, currently, at least 3 implementations based on the Progol algorithm: CProgol (by S. Muggleton, written in C which contains its own Prolog interpreter), Indlog (by Rui Camacho, in Prolog requiring the Yap compiler) and P-Progol (by Ashwin Srinivasan, in Prolog largely developed with Yap). The main differences in implementation (other than language) are in the search technique used and degree of user-interaction allowed

The basic P-Progol algorithm
During routine use, P-Progol follows a very simple procedure that can be described in 4 steps:
Select example. Select an example to be generalised. If none exist, stop, otherwise proceed to the next step.
Build most-specific-clause. Construct the most specific clause that entails the example selected, and is within language restrictions provided. This is usually a definite clause with many literals, and is called the "bottom clause." This step is sometimes called the "saturation" step.
Search. Find a clause more general than the bottom clause. This is done by searching for some subset of the literals in the bottom clause that has the "best" score. Two points should be noted. First, confining the search to subsets of the bottom clause does not produce all the clauses more general than it, but is good enough for this thumbnail sketch. Second, the exact nature of the score of a clause is not really important here. This step is sometimes called the "reduction" step.
Remove redundant. The clause with the best score is added to the current theory, and all examples made redundant are removed. This step is sometimes called the "cover removal" step. Note here that the best clause may make clauses other than the examples redundant. Again, this is ignored here. Return to Step 1.
P-Progol is an implementation based around these 4 steps. A more advanced use of P-Progol (see section Advanced use of P-Progol) allows alteration to each of these steps.


\subsection{FOIL}
\nocite{Quinlan:1993:FMR:645323.649599}
FOIL è un sistema di apprendimento in grado di costruire delle clausole di Horn a partire da esempi sia positivi che negativi.
%FOIL is a learning system that constructs Horn clause programs from examples.
%FOIL is a system for learning function-free Horn clause definitions of a relation in terms of itself and other relations.

The principal differences between zeroth-order and first-order supervised learning systems are the form of the training data and the way that a learned theory is expressed. Data for zeroth-order learning programs such as CN2 and C4.5 Quinlan, 1992 comprise preclassi ed cases, each described by its values for a xed collection of attributes.
These systems develop theories, in the form of decision trees or production rules, that relate a case's class to its attribute values. In contrast, the input to first-order learners (usually) contains ground assertions about a number of multi-argument predicates or relations and the learned theory consists of a logic program, restricted to Horn clauses or something similar, that predicts when a vector of arguments will satisfy a designated predicate.
FOIL uses a divide-and-cover strategy adapted from zeroth-order learning. These approaches have proved to be more e cient and robust, enabling larger training sets to be analysed to learn more complex programs.

The program is actually slightly more exible since it can learn several relations in sequence, allows negated literals in the de nitions (using standard Prolog semantics), and can employ certain constants in the de nitions it produces.
FOIL 's input consists of information about the relations, one of which (the target
relation ) is to be de ned by a Horn clause program. For each relation it is given
a set of tuples of constants that belong to the relation. For the target relation
it might also be given tuples that are known not to belong to the relation;
alternatively, the closed world assumption may be invoked to state that no tuples,
other than those speci ed, belong to the target relation. Tuples known to be in
the target relation will be referred to as tuples and those not in the relation as
tuples. The learning task is then to nd a set of clauses for the target relation
that accounts for all the tuples while not covering any of the tuples.
The basic approach used by FOIL is an AQ-like covering algorithm Michalski,
Mozetic, Hong and Lavrac, 1986]. It starts with a training set containing all
and tuples, constructs a function-free Horn clause to `explain' some of the
tuples, removes the covered tuples from the training set, and continues with
the search for the next clause. When clauses covering all the tuples have been
found, they are reviewed to eliminate any redundant clauses and reordered so
that any recursive clauses come after the non-recursive base cases.
Perfect de nitions that exactly match the data are not always possible, particu-
larly in real-world situations where incorrect values and missing tuples are to be
expected. To get around this problem, FOIL uses encoding-length heuristics tolimit the complexity of clauses and programs. The nal clauses may cover most
(rather than all) of the tuples while covering few (rather than none) of the
tuples. See Quinlan, 1990] for details.
